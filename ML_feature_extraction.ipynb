{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling\n",
    "\n",
    "Min-Max scaling, also known as Min-Max normalization, is a data preprocessing technique used to rescale the values of features to a specific range, typically \\([0, 1]\\). This is particularly useful in preparing data for machine learning algorithms that are sensitive to the scale of input data, such as those based on distance metrics (e.g., k-nearest neighbors, support vector machines).\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The Min-Max scaling formula is as follows:\n",
    "\n",
    "$  x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}  $\n",
    "\n",
    "where:\n",
    "- \\( x \\) is the original value.\n",
    "- $ \\min(x) $ is the minimum value in the feature.\n",
    "- $ \\max(x) $ is the maximum value in the feature.\n",
    "- $ x' $ is the scaled value.\n",
    "\n",
    "This formula scales each feature to the range \\([0, 1]\\). However, the range can be adjusted to any desired range \\([a, b]\\) using the following formula:\n",
    "\n",
    "$  x' = a + \\frac{(x - \\min(x)) \\times (b - a)}{\\max(x) - \\min(x)}  $\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Uniform Scale**: Rescales features to a uniform range, ensuring that all features contribute equally to the model.\n",
    "- **Improved Convergence**: Helps algorithms converge faster by providing a consistent scale.\n",
    "- **Compatibility**: Suitable for algorithms that require input data within a certain range.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- **Sensitive to Outliers**: Since Min-Max scaling depends on the minimum and maximum values, it can be affected by outliers.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's illustrate Min-Max scaling with an example in Python using `sklearn.preprocessing.MinMaxScaler`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     total_bill   tip\n",
      "0         16.99  1.01\n",
      "1         10.34  1.66\n",
      "2         21.01  3.50\n",
      "3         23.68  3.31\n",
      "4         24.59  3.61\n",
      "..          ...   ...\n",
      "239       29.03  5.92\n",
      "240       27.18  2.00\n",
      "241       22.67  2.00\n",
      "242       17.82  1.75\n",
      "243       18.78  3.00\n",
      "\n",
      "[244 rows x 2 columns]\n",
      "     Scaled_total_bill  Scaled_tip\n",
      "0             0.291579    0.001111\n",
      "1             0.152283    0.073333\n",
      "2             0.375786    0.277778\n",
      "3             0.431713    0.256667\n",
      "4             0.450775    0.290000\n",
      "..                 ...         ...\n",
      "239           0.543779    0.546667\n",
      "240           0.505027    0.111111\n",
      "241           0.410557    0.111111\n",
      "242           0.308965    0.083333\n",
      "243           0.329074    0.222222\n",
      "\n",
      "[244 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df=sns.load_dataset(\"tips\")\n",
    "\n",
    "scaler= MinMaxScaler()\n",
    "print(df[['total_bill','tip']])\n",
    "df1=pd.DataFrame(scaler.fit_transform(df[['total_bill','tip']]),columns=['Scaled_total_bill','Scaled_tip'])\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Vector Technique in Feature Scaling\n",
    "\n",
    "The Unit Vector technique, also known as normalization or vector normalization, scales each feature vector (row of the data) to have a unit norm (length). This technique ensures that each data point lies on the surface of a hypersphere. The most common norm used for normalization is the \\($L^2$\\) norm, but other norms like \\($L^1$\\) can also be used.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "For a given feature vector \\($\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$\\), the normalized vector \\($\\mathbf{x'}$\\) is calculated as follows:\n",
    "\n",
    "$ \\mathbf{x'} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|} $\n",
    "\n",
    "where \\|\\mathbf{x}\\| is the norm of the vector  $ \\mathbf{x} $ . For the  $ L^2 $  norm, this is:\n",
    "\n",
    "$ \\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2} $ \n",
    "\n",
    "Thus, the \\( $ L^2 $ \\) normalized vector is:\n",
    "\n",
    "  $ x_i' = \\frac{x_i}{\\sqrt{\\sum_{j=1}^n x_j^2}} $  \n",
    "\n",
    "### Differences from Min-Max Scaling\n",
    "\n",
    "- **Min-Max Scaling**: Transforms features to a fixed range, typically \\([0, 1]\\) or \\([-1, 1]\\), based on the minimum and maximum values of each feature.\n",
    "- **Unit Vector Scaling**: Normalizes each data point independently to have a unit norm, ensuring that the length of each feature vector is 1. It focuses on the direction of the data points rather than their range.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's illustrate Unit Vector scaling with an example in Python using `sklearn.preprocessing.Normalizer`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width\n",
      "0             5.1          3.5           1.4          0.2\n",
      "1             4.9          3.0           1.4          0.2\n",
      "2             4.7          3.2           1.3          0.2\n",
      "3             4.6          3.1           1.5          0.2\n",
      "4             5.0          3.6           1.4          0.2\n",
      "..            ...          ...           ...          ...\n",
      "145           6.7          3.0           5.2          2.3\n",
      "146           6.3          2.5           5.0          1.9\n",
      "147           6.5          3.0           5.2          2.0\n",
      "148           6.2          3.4           5.4          2.3\n",
      "149           5.9          3.0           5.1          1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "     sepal_length  sepal_width  petal_length  petal_width\n",
      "0        0.803773     0.551609      0.220644     0.031521\n",
      "1        0.828133     0.507020      0.236609     0.033801\n",
      "2        0.805333     0.548312      0.222752     0.034269\n",
      "3        0.800030     0.539151      0.260879     0.034784\n",
      "4        0.790965     0.569495      0.221470     0.031639\n",
      "..            ...          ...           ...          ...\n",
      "145      0.721557     0.323085      0.560015     0.247699\n",
      "146      0.729654     0.289545      0.579090     0.220054\n",
      "147      0.716539     0.330710      0.573231     0.220474\n",
      "148      0.674671     0.369981      0.587616     0.250281\n",
      "149      0.690259     0.350979      0.596665     0.210588\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "df=sns.load_dataset(\"iris\")\n",
    "print(df[['sepal_length','sepal_width','petal_length','petal_width']])\n",
    "df1=pd.DataFrame(normalize(df[['sepal_length','sepal_width','petal_length','petal_width']]),columns=['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms a high-dimensional dataset into a lower-dimensional one while retaining most of the original variance. PCA achieves this by identifying the directions (principal components) along which the variance in the data is maximized.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Standardization**: Standardize the data to have a mean of 0 and a standard deviation of 1 for each feature.\n",
    "\n",
    "2. **Covariance Matrix Computation**: Compute the covariance matrix to understand the relationships between different features.\n",
    "\n",
    "3. **Eigen Decomposition**: Perform eigen decomposition on the covariance matrix to find the eigenvalues and eigenvectors. The eigenvectors represent the directions of the principal components, and the eigenvalues represent the magnitude of variance in these directions.\n",
    "\n",
    "4. **Sorting Eigenvalues**: Sort the eigenvalues in descending order and select the top \\(k\\) eigenvectors corresponding to the largest eigenvalues. These eigenvectors form the principal components.\n",
    "\n",
    "5. **Projection**: Project the original data onto the new \\(k\\)-dimensional subspace formed by the principal components.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Dimensionality Reduction**: Reduces the number of features while retaining most of the variance, simplifying the dataset.\n",
    "- **Noise Reduction**: Helps in removing noise and redundant features.\n",
    "- **Improved Performance**: Can improve the performance of machine learning algorithms by reducing overfitting and computational complexity.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's illustrate PCA with an example in Python using `sklearn.decomposition.PCA`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    feature1  feature2\n",
      "0       2.5       2.4\n",
      "1       0.5       0.7\n",
      "2       2.2       2.9\n",
      "3       1.9       2.2\n",
      "4       3.1       3.0\n",
      "5       2.3       2.7\n",
      "6       2.0       1.6\n",
      "7       1.0       1.1\n",
      "8       1.5       1.6\n",
      "9       1.1       0.9\n",
      "\n",
      "PCA Data:\n",
      "         PC1       PC2\n",
      "0 -1.086432 -0.223524\n",
      "1  2.308937  0.178081\n",
      "2 -1.241919  0.501509\n",
      "3 -0.340782  0.169919\n",
      "4 -2.184290 -0.264758\n",
      "5 -1.160739  0.230481\n",
      "6  0.092605 -0.453317\n",
      "7  1.482108  0.055667\n",
      "8  0.567226  0.021305\n",
      "9  1.563287 -0.215361\n",
      "\n",
      "Explained Variance Ratio: [0.96296464 0.03703536]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'feature1': [2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1],\n",
    "    'feature2': [2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame for the PCA data\n",
    "pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(\"Original Data:\\n\", df)\n",
    "print(\"\\nPCA Data:\\n\", pca_df)\n",
    "print(\"\\nExplained Variance Ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship Between PCA and Feature Extraction\n",
    "\n",
    "Principal Component Analysis (PCA) is closely related to feature extraction. Feature extraction involves transforming the original features into a new set of features that capture the most important information in the data. PCA achieves this by identifying and projecting the data onto the principal components, which are the directions of maximum variance in the data.\n",
    "\n",
    "### How PCA is Used for Feature Extraction\n",
    "\n",
    "1. **Variance Maximization**: PCA identifies the directions (principal components) along which the variance in the data is maximized.\n",
    "2. **Dimensionality Reduction**: By selecting a subset of the principal components, PCA reduces the number of features while retaining most of the original variance.\n",
    "3. **New Feature Space**: The principal components form a new feature space where each component is a linear combination of the original features. These new features are uncorrelated and capture the most significant patterns in the data.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's illustrate how PCA can be used for feature extraction with an example in Python using `sklearn.decomposition.PCA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    feature1  feature2  feature3\n",
      "0       2.5       2.4       3.5\n",
      "1       0.5       0.7       0.2\n",
      "2       2.2       2.9       3.2\n",
      "3       1.9       2.2       2.9\n",
      "4       3.1       3.0       4.1\n",
      "5       2.3       2.7       3.3\n",
      "6       2.0       1.6       3.0\n",
      "7       1.0       1.1       1.5\n",
      "8       1.5       1.6       2.5\n",
      "9       1.1       0.9       1.4\n",
      "\n",
      "PCA Data:\n",
      "         PC1       PC2\n",
      "0 -1.373427  0.202617\n",
      "1  3.103600 -0.339408\n",
      "2 -1.338163 -0.566786\n",
      "3 -0.451958 -0.113369\n",
      "4 -2.578542  0.110996\n",
      "5 -1.326280 -0.276351\n",
      "6 -0.156429  0.584672\n",
      "7  1.756273  0.019510\n",
      "8  0.493276  0.151515\n",
      "9  1.871650  0.226603\n",
      "\n",
      "Explained Variance Ratio: [0.95922786 0.03318559]\n",
      "\n",
      "Principal Components:\n",
      " [[-0.5824797  -0.56946131 -0.58002691]\n",
      " [ 0.33491824 -0.81832751  0.46708657]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'feature1': [2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1],\n",
    "    'feature2': [2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9],\n",
    "    'feature3': [3.5, 0.2, 3.2, 2.9, 4.1, 3.3, 3.0, 1.5, 2.5, 1.4]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame for the PCA data\n",
    "pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(\"Original Data:\\n\", df)\n",
    "print(\"\\nPCA Data:\\n\", pca_df)\n",
    "print(\"\\nExplained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"\\nPrincipal Components:\\n\", pca.components_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Min-Max Scaling to Preprocess Data for a Food Delivery Recommendation System\n",
    "\n",
    "In a food delivery recommendation system, the dataset may contain features such as price, rating, and delivery time. These features can have different ranges and units, which may affect the performance of machine learning algorithms. To ensure that all features contribute equally to the model, we can use Min-Max scaling to preprocess the data.\n",
    "\n",
    "### Steps for Min-Max Scaling\n",
    "\n",
    "1. **Identify Features**: Identify the features to be scaled, such as price, rating, and delivery time.\n",
    "2. **Compute Minimum and Maximum Values**: Calculate the minimum and maximum values for each feature.\n",
    "3. **Apply Min-Max Scaling**: Use the Min-Max scaling formula to transform the values of each feature to the range [0, 1].\n",
    "\n",
    "### Min-Max Scaling Formula\n",
    "\n",
    "\\[ $ x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} $ \\]\n",
    "\n",
    "where:\n",
    "- \\($ x $\\) is the original value.\n",
    "- \\( $ \\min(x)  $\\) is the minimum value in the feature.\n",
    "- \\( $ \\max(x) $ \\) is the maximum value in the feature.\n",
    "- \\( $ x' $ \\) is the scaled value.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's illustrate Min-Max scaling with an example dataset containing price, rating, and delivery time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    price  rating  delivery_time\n",
      "0     10     4.2             30\n",
      "1     15     4.5             25\n",
      "2     20     4.0             35\n",
      "3     25     3.8             40\n",
      "4     30     4.7             20\n",
      "\n",
      "Scaled Data:\n",
      "    price    rating  delivery_time\n",
      "0   0.00  0.444444           0.50\n",
      "1   0.25  0.777778           0.25\n",
      "2   0.50  0.222222           0.75\n",
      "3   0.75  0.000000           1.00\n",
      "4   1.00  1.000000           0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'price': [10, 15, 20, 25, 30],\n",
    "    'rating': [4.2, 4.5, 4.0, 3.8, 4.7],\n",
    "    'delivery_time': [30, 25, 35, 40, 20]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Create a DataFrame for the scaled data\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "print(\"Original Data:\\n\", df)\n",
    "print(\"\\nScaled Data:\\n\", scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA to Reduce Dimensionality in Stock Price Prediction\n",
    "\n",
    "In a stock price prediction project, the dataset may contain numerous features, such as company financial data and market trends. High-dimensional data can lead to issues like overfitting, increased computational complexity, and difficulty in visualizing the data. Principal Component Analysis (PCA) can be used to reduce the dimensionality of the dataset while retaining most of the variance.\n",
    "\n",
    "### Steps for Using PCA\n",
    "\n",
    "1. **Standardize the Data**: Standardize the features to have a mean of 0 and a standard deviation of 1.\n",
    "2. **Compute the Covariance Matrix**: Calculate the covariance matrix to understand the relationships between features.\n",
    "3. **Perform Eigen Decomposition**: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. **Select Principal Components**: Choose the top \\(k\\) principal components that explain the most variance.\n",
    "5. **Transform the Data**: Project the original data onto the new feature space defined by the selected principal components.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's illustrate how PCA can be used to reduce the dimensionality of a stock price prediction dataset with an example in Python using `sklearn.decomposition.PCA`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    revenue  profit  market_cap  debt  dividend  interest_rate  inflation_rate  \\\n",
      "0      100      10         500    50       2.0            1.5             2.1   \n",
      "1      150      15         700    60       2.5            1.7             2.3   \n",
      "2      200      20         800    70       3.0            1.8             2.2   \n",
      "3      250      25         900    80       3.5            1.9             2.4   \n",
      "4      300      30        1000    90       4.0            2.0             2.5   \n",
      "\n",
      "   gdp_growth  \n",
      "0         3.5  \n",
      "1         3.6  \n",
      "2         3.7  \n",
      "3         3.8  \n",
      "4         3.9  \n",
      "\n",
      "PCA Data:\n",
      "         PC1       PC2\n",
      "0  4.150439  0.065120\n",
      "1  1.593253 -0.581157\n",
      "2  0.151408  0.681863\n",
      "3 -1.992580 -0.049597\n",
      "4 -3.902520 -0.116229\n",
      "\n",
      "Explained Variance Ratio: [0.97468911 0.02057227]\n",
      "\n",
      "Principal Components:\n",
      " [[-0.35714391 -0.35714391 -0.35544647 -0.35714391 -0.35714391 -0.35544647\n",
      "  -0.33099357 -0.35714391]\n",
      " [ 0.14510176  0.14510176  0.07327591  0.14510176  0.14510176  0.07327591\n",
      "  -0.94020672  0.14510176]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data representing financial and market features\n",
    "data = {\n",
    "    'revenue': [100, 150, 200, 250, 300],\n",
    "    'profit': [10, 15, 20, 25, 30],\n",
    "    'market_cap': [500, 700, 800, 900, 1000],\n",
    "    'debt': [50, 60, 70, 80, 90],\n",
    "    'dividend': [2, 2.5, 3, 3.5, 4],\n",
    "    'interest_rate': [1.5, 1.7, 1.8, 1.9, 2.0],\n",
    "    'inflation_rate': [2.1, 2.3, 2.2, 2.4, 2.5],\n",
    "    'gdp_growth': [3.5, 3.6, 3.7, 3.8, 3.9]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data=scaler.fit_transform(df)\n",
    "\n",
    "pca=PCA(n_components=2)\n",
    "\n",
    "pca_data= pca.fit_transform(scaled_data)\n",
    "\n",
    "pca_df=pd.DataFrame(pca_data,columns=['PC1','PC2'])\n",
    "\n",
    "print(\"Original Data:\\n\", df)\n",
    "print(\"\\nPCA Data:\\n\", pca_df)\n",
    "print(\"\\nExplained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"\\nPrincipal Components:\\n\", pca.components_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling to Transform Values to a Range of -1 to 1\n",
    "\n",
    "Given a dataset containing the values: $[1, 5, 10, 15, 20]$, we will perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "### Min-Max Scaling Formula\n",
    "\n",
    "To scale the values to a range of $[-1, 1]$, we use the following formula:\n",
    "\n",
    "$ x' = \\frac{(x - \\min(x)) \\cdot (new_{max} - new_{min})}{\\max(x) - \\min(x)} + new_{min} $\n",
    "\n",
    "where:\n",
    "- $ x $ is the original value.\n",
    "- $ \\min(x) $ is the minimum value in the dataset.\n",
    "- $ \\max(x) $ is the maximum value in the dataset.\n",
    "- $ new_{min} $ is the new minimum value (-1).\n",
    "- $ new_{max} $ is the new maximum value (1).\n",
    "- $ x' $ is the scaled value.\n",
    "\n",
    "### Calculation\n",
    "\n",
    "Let's apply this formula to each value in the dataset.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -0.57894737, -0.05263158,  0.47368421,  1.        ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Min and max values of the original data\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "\n",
    "# New min and max values for the scaled data\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Min-Max scaling function\n",
    "def min_max_scale(x, min_val, max_val, new_min, new_max):\n",
    "    return ((x - min_val) * (new_max - new_min) / (max_val - min_val)) + new_min\n",
    "\n",
    "# Apply Min-Max scaling to each value\n",
    "scaled_data = min_max_scale(data, min_val, max_val, new_min, new_max)\n",
    "\n",
    "scaled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Number of Principal Components for Feature Extraction using PCA\n",
    "\n",
    "When performing Feature Extraction using PCA on a dataset containing features like height, weight, age, gender, and blood pressure, the number of principal components to retain depends on several factors, including the desired level of dimensionality reduction and the variance explained by each principal component.\n",
    "\n",
    "### Steps to Decide Number of Principal Components\n",
    "\n",
    "1. **Standardize the Data**: Ensure all features are standardized to have a mean of 0 and a standard deviation of 1.\n",
    "  \n",
    "2. **Compute PCA**: Apply PCA to the standardized data and obtain the explained variance ratio for each principal component.\n",
    "\n",
    "3. **Cumulative Explained Variance**: Calculate the cumulative explained variance and choose the number of principal components that explain a significant portion (e.g., 95%) of the total variance.\n",
    "\n",
    "### Example Consideration\n",
    "\n",
    "Let's hypothesize that after performing PCA on the dataset, we obtain the following explained variance ratios for each principal component:\n",
    "\n",
    "- PC1 explains 70% of the variance.\n",
    "- PC2 explains 20% of the variance.\n",
    "- PC3 explains 5% of the variance.\n",
    "- PC4 explains 3% of the variance.\n",
    "- PC5 explains 2% of the variance.\n",
    "\n",
    "In this scenario, the first two principal components (PC1 and PC2) collectively explain \\( 70\\% + 20\\% = 90\\% \\) of the variance. Choosing to retain these two components would provide a good balance between dimensionality reduction and retaining significant information from the original dataset.\n",
    "\n",
    "### Decision Criteria\n",
    "\n",
    "- **Threshold**: A common practice is to set a threshold, such as retaining principal components that cumulatively explain at least 95% of the variance.\n",
    "  \n",
    "- **Visualization**: Consider visualizing the data in reduced dimensions (e.g., using scatter plots of principal components) to assess if the retained components capture meaningful patterns.\n",
    "\n",
    "- **Application**: Evaluate how the reduced dataset performs in downstream tasks like prediction or clustering.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The decision on how many principal components to retain in PCA for feature extraction depends on the specific goals of the analysis, the trade-off between dimensionality reduction and information retention, and the variance explained by each principal component. In general, retaining principal components that collectively explain a high percentage of the variance ensures that important patterns and relationships in the data are preserved while reducing the complexity of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    height  weight  age  gender  blood_pressure\n",
      "0     160      60   25       0             120\n",
      "1     165      65   30       1             130\n",
      "2     170      70   35       0             125\n",
      "3     175      75   40       1             140\n",
      "4     180      80   45       1             135\n",
      "\n",
      "PCA-transformed Data:\n",
      "         PC1       PC2\n",
      "0  3.080633 -0.098238\n",
      "1  0.690610  1.283370\n",
      "2  0.773679 -1.179750\n",
      "3 -1.933855  0.465176\n",
      "4 -2.611067 -0.470558\n",
      "\n",
      "Explained Variance Ratio:\n",
      " [8.44931430e-01 1.39452568e-01 1.56160014e-02 2.57855783e-33\n",
      " 5.48726886e-36]\n",
      "\n",
      "Cumulative Explained Variance Ratio:\n",
      " [0.84493143 0.984384   1.         1.         1.        ]\n",
      "\n",
      "Number of Principal Components Selected: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'height': [160, 165, 170, 175, 180],\n",
    "    'weight': [60, 65, 70, 75, 80],\n",
    "    'age': [25, 30, 35, 40, 45],\n",
    "    'gender': [0, 1, 0, 1, 1],\n",
    "    'blood_pressure': [120, 130, 125, 140, 135]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA and transform the data\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Choosing number of principal components to retain (e.g., cumulative explained variance of 95%)\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "num_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "# Selecting top principal components\n",
    "pca_selected = PCA(n_components=num_components)\n",
    "pca_data_selected = pca_selected.fit_transform(scaled_data)\n",
    "\n",
    "# Creating a DataFrame for the PCA-transformed data\n",
    "pca_df = pd.DataFrame(data=pca_data_selected, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "\n",
    "# Display results\n",
    "print(\"Original Data:\\n\", df)\n",
    "print(\"\\nPCA-transformed Data:\\n\", pca_df)\n",
    "print(\"\\nExplained Variance Ratio:\\n\", explained_variance_ratio)\n",
    "print(\"\\nCumulative Explained Variance Ratio:\\n\", cumulative_variance_ratio)\n",
    "print(\"\\nNumber of Principal Components Selected:\", num_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
